<html> <head> 

<meta http-equiv="Content-Type" content="text/html"> 
<meta name="description" content="***"> 
<meta name="keywords" content="Rui Wang, Shanghai Jiao Tong Univerisity, Machine Translation, NLP> 
<meta charset="UTF-8">

<title> Rui Wang, Shanghai Jiao Tong Univerisity, Machine Translation, NLP</title>

</head> 

<body>
<table>
<tr><th></th><th></th><th></th>
	
<tr>
<td><img src="image-2_1920x2560.jpeg" width=150 height=200 alt="a photo"></td>	
<td>&nbsp;&nbsp;&nbsp;</td>

<td><h1> Rui Wang </h1>
</p>
<strong>Associate Professor & Ph.D. Advisor </strong></br>
Department of Computer Science and Engineering</br>
Shanghai Jiao Tong University</br>
</br>

Email: wangrui12 (as you know) sjtu.edu.cn
</td>

</table>
<hr>
<h3>Biography</h3>
I am a computational linguist working at Shanghai Jiao Tong University since 2021. Previously, I worked at the Japanese National Institute of Information and Communications Technology from 2016 to 2020. I was a joint Ph.D. student at Shanghai Jiao Tong University and the French National Centre for Scientific Research from 2012 to 2016. </p> 

Language intelligence is a form of intelligence in which humans (and machines) learn about the outside world by reading natural language, producing highly abstract linguistic thinking; and then understanding, improving, and creating about the outside world. We aim to explore and extend the boundaries of language intelligence in the following areas: <br/>

1. <i>The mechanism and application of LLM. </i><br/>
2. <i>Language Intelligence for Science: primarily linguistic, psychology, cognitive science, etc.</i> <br/>
3. <i>Machine Translation: I worked on it for over ten years and will never give it up. </i><br/>

<hr>

<h3> Language Intelligence and Computational Linguistic Lab (MT Lab)</h3>
I am always fortunate to work with these brilliant young researchers. (Previously Machine Translation Lab from 2017-2022)<br/>
<font color="red">目前实验室主要由NLP背景的同学为主，我们亦欢迎有心理学、语言学、认知科学等交叉学科背景的同学加入实验室。 </font>
<font color="red">目前有一个2024年考研硕士名额，欢迎咨询。 <br/></font>

<h4>@SJTU</h4>
Ph.D. Students: </p>
Qingyuan Tian (2024-)</br>	
Yang Han (2023-)</br>									
<a class="grey" href="https://alsace08.github.io/cv/index.html">Yiming Wang</a> (2023-)</br>
Xingyu Chen (2022-)</br>
<a class="grey" href="https://zwhe99.github.io/">Zhiwei He</a> (2021-)</br>

</p>
Master Students: </p>

2023-: <a class="grey" href="https://geralt-targaryen.github.io/">Ziyin Zhang</a>, Xiaofeng Wang, Lizhen Xu, Sheng Li </br>
2022-: <a class="grey" href="https://scholar.google.com/citations?user=a3UulbMAAAAJ&hl=zh-CN">Hongkun Hao</a>, Yiming Ai, Tianxiang Hu, <a class="grey" href="https://scholar.google.com/citations?hl=zh-CN&user=psCdg8EAAAAJ">Wenhong Zhu</a>, Tian Xia</br>
2021-: <a class="grey" href="https://scholar.google.com/citations?user=iaJtocIAAAAJ&hl=zh-CN">Ruize Gao</a> </br>

</p>
Undergraduate Students: </p>
2022: Yushen Chen (-->Master Student, SJTU)</br>
2021: Xiaoyi Bao (-->Microsoft), Ruiyi Wang (-->Master Student, LTI, CMU)</br>


<h4>@NICT</h4>
Interns: </p>
<a class="grey" href="http://bcmi.sjtu.edu.cn/home/zhangzs/">Zhuosheng Zhang</a> (Matser Student, SJTU, 2019-2020)</br>
<a class="grey" href="https://scholar.google.com/citations?user=PxC3X3QAAAAJ&hl=zh-CN">Haipeng Sun</a> (Ph.D. Student, HIT, 2018-2020)</br>
<a class="grey" href="https://zzsfornlp.github.io/">Zhisong Zhang</a> (Matser Student, SJTU, 2017-2018)</br>
<a class="grey" href="https://chenkehai.github.io/">Kehai Chen</a> (Ph.D. Student, HIT, 2017-2018)</br>

<hr>
	
<h3>Academic Services</h3>
Program Chair: MT Summit 2023 (Research Track)<br/>
Senior Area Chairs: ACL-2024<br/>
Area Chairs: ICML (2023-), NeurIPS (2022-), ICLR (2021-), EMNLP (2022), NAACL (2021), CoNLL (2021-2022), CCL (2018-2019) <br/>
Associate Editor: ACL Rolling Review (2024-), IEICE Transactions on Information and Systems (2022-2024)<br/>
Standing Reviewers: CL (2020-) and TACL (2020-)<br/>


<hr>
<h3>Shared Tasks</h3>
<a class="grey" href="https://www2.statmt.org/wmt23/word-autocompletion.html">WMT-2023</a>: 1st places in Word-Level AutoCompletion<a href="https://www2.statmt.org/wmt23/pdf/2023.wmt-1.77.pdf">[Paper]</a><br/>
<a class="grey" href="http://www.statmt.org/wmt22/translation-task.html">WMT-2022</a>: 1st places in Livonian<->English<a href="https://aclanthology.org/2022.wmt-1.18/">[Paper]</a><br/>
<a class="grey" href="">IWSLT-2022</a>: 1st in the Simultaneous Speech Translation task <a href="https://aclanthology.org/2022.iwslt-1.10v2.pdf">[Results]</a><a href="https://aclanthology.org/2022.iwslt-1.16/">[Paper]</a> <br/>
<a class="grey" href="http://www.statmt.org/wmt20/translation-task.html">WMT-2020</a>: 1st in three tasks (English->Chinese, Polish->English, and  German-Upper Sorbian) <a href="http://matrix.statmt.org/matrix/systems_list/1920">[Results]</a><a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.22.pdf">[Paper]</a> <br/>
<a class="grey" href="http://mrp.nlpl.eu/">CoNLL-2019</a>: 1st in the DM sub-task and the 2nd overall <a href="https://docs.google.com/spreadsheets/d/14s5Y_vM8YZ-g7O88EKnJ85c7HH1EXPQizvH3z0wDo80/edit#gid=0">[Results]</a><a href="https://www.aclweb.org/anthology/K19-2004/">[Paper]</a> <br/>

<a class="grey" href="http://www.statmt.org/wmt19/translation-task.html">WMT-2019</a>: 1st in the only unsupervised MT task (German-Czech) <a class="grey" href="http://matrix.statmt.org/matrix/systems_list/1897">[Results]</a> <a class="grey" href="https://www.aclweb.org/anthology/W19-5330/">[Paper]</a> <br/>
<a class="grey" href="http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/index.html">WAT-2018</a>: 1st places  in Myanmar (Burmese) <- English <a class="grey" href="http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/list.php?t=27&o=8">[Results]</a><a class="grey" href="wat.pdf">[Paper]</a><br/>

<a class="grey" href="http://www.statmt.org/wmt18/translation-task.html">WMT-2018</a>: 1st places  in four tasks (English<->Estonian and English<->Finnish) <a class="grey" href="wmt-newstest18-results.pdf">[Results]</a><a class="grey" href="http://www.statmt.org/wmt18/pdf/WMT046.pdf">[Paper]</a> </p>
	
<hr>
	
<h3>Teaching</h3>
<h4>Lecture</h4>
CS3966: Natural Language Processing and Large Language Model, 2024-<br/>
	
CS3602: Natural Language Processing, 2021-<br/>

CS438: Information Extraction, 2021-2023<br/>

CS247: Data Mining, 2021-2022</p>



	
<h4>Tutorial</h4>
<a href="https://wangruinlp.github.io/unmt.html"><i>Advances and Challenges in Unsupervised Neural Machine Translation</i></a>. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;<strong>Rui Wang</strong> and Hai Zhao<br/>
&nbsp;&nbsp;&nbsp;&nbsp;16th conference of the European Chapter of the Association for Computational Linguistics (<strong>EACL-Tutorial</strong>), 2021<br/>

<a href="https://aclanthology.org/2021.emnlp-tutorials.6/"><i>Syntax in End-to-End Natural Language Processing</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Hai Zhao, <strong>Rui Wang</strong>, and Kehai Chen<br/>
&nbsp;&nbsp;&nbsp;&nbsp;The 2021 Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP-Tutorial</strong>), 2021<br/>

<a href="https://wangruinlp.github.io/ccmt.pdf"><i>Domain Adaptation for Neural Machine Translation</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Chenhui Chu and <strong>Rui Wang</strong><br/>
&nbsp;&nbsp;&nbsp;&nbsp;The 16th China Conference on Machine Translation (<strong>CCMT-Tutorial</strong>), 2019<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<i>--You also refer to our survey paper in <a class="grey" href="https://www.aclweb.org/anthology/C18-1111/"><i>[COLING-2018]</i></a> </i></p>


<hr>

<h3>Recent Publication <a class="grey" href="https://scholar.google.com/citations?user=oTU0v5IAAAAJ&hl">[Google Scholar]</a> <a class="grey" href="https://wangruinlp.github.io/pub.html">[Full Publication]</a> </p></h3>
<i>Note: If you have a technical question about a research paper, it is best to try to get in contact with all of the authors, so the most appropriate person can respond as quickly as possible.</i><br/>

<h4>Preprint</h4>
<a class="grey" href="https://arxiv.org/abs/2402.18223"><i>Improving Open-Ended Text Generation via Adaptive Decoding</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Wenhong Zhu, Hongkun Hao, Zhiwei He, Yiming Ai, Yang Yang, <strong>Rui Wang</strong><br/>
	
<a class="grey" href="https://arxiv.org/abs/2402.15813"><i>Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, <strong>Rui Wang</strong><br/>
	

	
<a class="grey" href="https://arxiv.org/abs/2402.14007"><i>Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, <strong>Rui Wang</strong><br/>
	

<a class="grey" href="https://arxiv.org/abs/2402.14679"><i>Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, <strong>Rui Wang</strong><br/>
	
<a class="grey" href="https://arxiv.org/abs/2401.00246"><i>Boosting Large Language Model for Speech Synthesis: An Empirical Study</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, <strong>Rui Wang</strong>, Furu Wei<br/>

	
<a class="grey" href="https://arxiv.org/abs/2311.09033"><i>MELA: Multilingual Evaluation of Linguistic Acceptability</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, <strong>Rui Wang</strong>, Hai Hu<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/sjtu-compling/MELA">[Github]</a><br/>

	
<a class="grey" href="https://arxiv.org/abs/2311.07989"><i>A Survey on Language Models for Code</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li,  and <strong>Rui Wang</strong><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/codefuse-ai/Awesome-Code-LLM">[Github]</a><a class="grey" href="https://huggingface.co/papers/2311.07989">[HuggingFace]</a><br/>

<a class="grey" href="https://arxiv.org/pdf/2311.11797.pdf"><i>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, <strong>Rui Wang</strong>, Gongshen Liu, Hai Zhao<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/Zoeyyao27/CoT-Igniting-Agent">[Github]</a><br/>
	
<a class="grey" href="https://arxiv.org/abs/2306.17820"><i>Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Yiming Wang, Zhuosheng Zhang, and <strong>Rui Wang</strong><br/>


<a class="grey" href="https://arxiv.org/abs/2305.14091"><i>Revisiting Acceptability Judgements: CoLAC - Corpus of Linguistic Acceptability in Chinese</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Hai Hu, Ziyin Zhang, Weifang Huang, Jackie Yan-Ki Lai, Aini Li, Yina Patterson, Jiahui Huang, Peng Zhang, Chien-Jer Charles Lin, <strong>Rui Wang</strong><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/huhailinguist/CoLAC">[Data (soon to appear)]</a><br/>
	

<a class="grey" href="https://arxiv.org/abs/2305.19118"><i>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, <strong>Rui Wang</strong>, Yujiu Yang, Zhaopeng Tu, Shuming Shi<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/Skytliang/Multi-Agents-Debate">[Code]</a><br/>


<h4>2024</h4>
<a class="grey" href="https://arxiv.org/abs/2311.09154"><i>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp; Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, <strong>Rui Wang*</strong>, Hongyuan Lu*<br/>
&nbsp;&nbsp;&nbsp;&nbsp;2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (<strong>NAACL</strong>), 2024<br/>

	
<a class="grey" href="https://arxiv.org/abs/2401.12873"><i>Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Zhiwei He, Xing Wang*, Wenxiang Jiao, Zhuosheng Zhang, <strong>Rui Wang*</strong>, Shuming Shi, Zhaopeng Tu<br/>
&nbsp;&nbsp;&nbsp;&nbsp;2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (<strong>NAACL</strong>), 2024<br/>

	
<i>Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents</i><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Hao Wang, Tang Li, Chenhui Chu, <strong>Rui Wang</strong>, and Pinpin Zhu<br/>
&nbsp;&nbsp;&nbsp;&nbsp;The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (<strong>LREC-COLING</strong>), 2024<br/>
	
<a class="grey" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00642/119992/Exploring-Human-Like-Translation-Strategy-with"><i>Exploring Human-Like Translation Strategy with Large Language Models</i></a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, <strong>Rui Wang*</strong>, Zhaopeng Tu*, Shuming Shi, Xing Wang<br/>
&nbsp;&nbsp;&nbsp;&nbsp;Transactions of the Association for Computational Linguistics (<strong>TACL</strong>), 2024<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a class="grey" href="https://github.com/zwhe99/MAPS-mt">[Code]</a><a class="grey" href="https://github.com/galaxyChen/WLAC-Joint-Training">[Code]</a><a class="grey" href="https://slator.com/how-large-language-models-mimic-human-translation-process/">[Press]</a><br/>

</details>
<hr>
</body> 
</html>
